{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOekhbEZmhp8IrKnS2zuaUC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathikprajapati/Summer-Analytics-Prathik/blob/main/KAGGLE_hackathon_File.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn tabulate lightgbm xgboost catboost\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('hacktrain.csv')\n",
        "test = pd.read_csv('hacktest.csv')\n",
        "test_id = test['ID']\n",
        "\n",
        "# Encode target\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(train['class'])\n",
        "\n",
        "# Drop ID and target\n",
        "X = train.drop(['ID', 'class'], axis=1)\n",
        "X_test = test.drop(['ID'], axis=1)\n",
        "\n",
        "# Imputation\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X = imputer.fit_transform(X)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "# Feature Engineering\n",
        "def engineer(X):\n",
        "    df = pd.DataFrame(X)\n",
        "    df['mean'] = df.mean(axis=1)\n",
        "    df['std'] = df.std(axis=1)\n",
        "    df['min'] = df.min(axis=1)\n",
        "    df['max'] = df.max(axis=1)\n",
        "    df['range'] = df['max'] - df['min']\n",
        "    df['skew'] = df.skew(axis=1)\n",
        "    return df.values\n",
        "\n",
        "X = engineer(X)\n",
        "X_test = engineer(X_test)\n",
        "\n",
        "# Train/Val Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "sample_weight = compute_sample_weight('balanced', y_train)\n",
        "\n",
        "# Models\n",
        "lgb = LGBMClassifier(n_estimators=500, learning_rate=0.03, num_leaves=60, class_weight='balanced', random_state=42)\n",
        "xgb = XGBClassifier(n_estimators=500, learning_rate=0.03, max_depth=6, scale_pos_weight=1, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
        "cat = CatBoostClassifier(n_estimators=500, learning_rate=0.03, depth=6, verbose=0, random_state=42)\n",
        "\n",
        "# Voting Ensemble\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[('lgb', lgb), ('xgb', xgb), ('cat', cat)],\n",
        "    voting='soft'  # use predicted probabilities\n",
        ")\n",
        "\n",
        "# Train\n",
        "ensemble.fit(X_train, y_train, sample_weight=sample_weight)\n",
        "\n",
        "# Validate\n",
        "y_pred = ensemble.predict(X_val)\n",
        "acc = accuracy_score(y_val, y_pred)\n",
        "f1 = f1_score(y_val, y_pred, average='macro')\n",
        "\n",
        "print(f\"✅ Accuracy: {acc:.4f}\")\n",
        "print(f\"✅ Macro F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Final Prediction\n",
        "final_preds = ensemble.predict(X_test)\n",
        "decoded_preds = label_encoder.inverse_transform(final_preds)\n",
        "\n",
        "# Save Submission\n",
        "submission = pd.DataFrame({'ID': test_id, 'class': decoded_preds})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"✅ Submission file created successfully!\")"
      ],
      "metadata": {
        "id": "Dm392c-FEZyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train['source'] = 'train'\n",
        "test['source'] = 'test'\n",
        "combined = pd.concat([train.drop('class', axis=1), test], axis=0)\n",
        "\n",
        "for col in train.columns.drop(['ID', 'class']):\n",
        "    plt.figure(figsize=(6,3))\n",
        "    sns.kdeplot(data=combined, x=col, hue='source')\n",
        "    plt.title(f'Distribution of {col} - Train vs Test')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "y_CDV191FPiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from scipy.stats import randint, uniform, ks_2samp\n",
        "from lightgbm import LGBMClassifier\n",
        "import warnings\n",
        "from lightgbm import early_stopping, log_evaluation\n",
        "import os\n",
        "\n",
        "os.environ['LOKY_MAX_CPU_COUNT'] = '6'\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"lightgbm\")\n",
        "\n",
        "# ===================== 🔹 Load Data =====================\n",
        "df_train = pd.read_csv(\"hacktrain.csv\")\n",
        "df_test = pd.read_csv(\"hacktest.csv\")\n",
        "\n",
        "# Backup ID for final submission\n",
        "ID = df_test['ID']\n",
        "\n",
        "# ===================== 🔹 Target Encode =====================\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(df_train['class'])\n",
        "\n",
        "X_train_df = df_train.drop(['ID', 'class'], axis=1)\n",
        "X_test_df = df_test.drop(['ID'], axis=1)\n",
        "\n",
        "# ===================== 🔹 Detect and Drop Shifted Features =====================\n",
        "X_train_df['__source'] = 0\n",
        "X_test_df['__source'] = 1\n",
        "combined = pd.concat([X_train_df, X_test_df], axis=0)\n",
        "\n",
        "shifted_features = []\n",
        "for col in X_train_df.columns:\n",
        "    if col == '__source': continue\n",
        "    stat, pval = ks_2samp(\n",
        "        combined[combined['__source'] == 0][col],\n",
        "        combined[combined['__source'] == 1][col]\n",
        "    )\n",
        "    if pval < 0.01:\n",
        "        shifted_features.append(col)\n",
        "\n",
        "print(f\"⚠️ Removing shifted features: {shifted_features}\")\n",
        "X_train_df.drop(columns=shifted_features + ['__source'], inplace=True)\n",
        "X_test_df.drop(columns=shifted_features + ['__source'], errors='ignore', inplace=True)\n",
        "\n",
        "# ===================== 🔹 Imputation =====================\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_imputed = imputer.fit_transform(X_train_df)\n",
        "X_test_imputed = imputer.transform(X_test_df)\n",
        "\n",
        "# ===================== 🔹 Feature Engineering =====================\n",
        "def add_stat_features(X):\n",
        "    df = pd.DataFrame(X)\n",
        "    df['mean'] = df.mean(axis=1)\n",
        "    df['std'] = df.std(axis=1)\n",
        "    df['min'] = df.min(axis=1)\n",
        "    df['max'] = df.max(axis=1)\n",
        "    df['range'] = df['max'] - df['min']\n",
        "    df['skew'] = df.skew(axis=1)\n",
        "    return df.values\n",
        "\n",
        "X_train_final = add_stat_features(X_train_imputed)\n",
        "X_test_final = add_stat_features(X_test_imputed)\n",
        "\n",
        "# ===================== 🔹 Polynomial Features =====================\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
        "X_train_final = poly.fit_transform(X_train_final)\n",
        "X_test_final = poly.transform(X_test_final)\n",
        "\n",
        "# ===================== 🔹 PCA =====================\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_final = pca.fit_transform(X_train_final)\n",
        "X_test_final = pca.transform(X_test_final)\n",
        "\n",
        "# ===================== 🔹 Train/Val Split =====================\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_final, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
        ")\n",
        "\n",
        "# ===================== 🔹 Sample Weights =====================\n",
        "sample_weight = compute_sample_weight('balanced', y_train_split)\n",
        "\n",
        "# ===================== 🔹 LGBM Tuning =====================\n",
        "model = LGBMClassifier(objective='multiclass', num_class=len(np.unique(y_train)), boosting_type='gbdt', random_state=42, verbose=-1)\n",
        "\n",
        "param_dist = {\n",
        "    'num_leaves': randint(30, 150),\n",
        "    'max_depth': [5, 7, 15],\n",
        "    'learning_rate': uniform(0.01, 0.1),\n",
        "    'n_estimators': randint(300, 800),\n",
        "    'subsample': uniform(0.6, 0.3),\n",
        "    'colsample_bytree': uniform(0.6, 0.3),\n",
        "    'reg_alpha': uniform(0.01, 0.5),\n",
        "    'reg_lambda': uniform(0.01, 0.5),\n",
        "    'min_child_samples': randint(5, 25),\n",
        "    'subsample_freq': [0, 1, 2]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,\n",
        "    scoring='f1_macro',\n",
        "    cv=cv,\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"🔍 Starting hyperparameter tuning...\")\n",
        "random_search.fit(X_train_split, y_train_split, sample_weight=sample_weight)\n",
        "\n",
        "# ===================== 🔹 Final Training =====================\n",
        "best_model = random_search.best_estimator_\n",
        "best_model.set_params(n_estimators=500, learning_rate=0.02)\n",
        "\n",
        "best_model.fit(\n",
        "    X_train_split, y_train_split,\n",
        "    sample_weight=sample_weight,\n",
        "    eval_set=[(X_val_split, y_val_split)],\n",
        "    eval_metric=\"multi_logloss\",\n",
        "    callbacks=[early_stopping(stopping_rounds=50), log_evaluation(50)]\n",
        ")\n",
        "\n",
        "# ===================== 🔹 Prediction & Submission =====================\n",
        "y_pred = best_model.predict(X_test_final)\n",
        "y_decoded = label_encoder.inverse_transform(y_pred)\n",
        "\n",
        "submission = pd.DataFrame({'ID': ID, 'class': y_decoded})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"✅ Submission file created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P18_KlrEF5aU",
        "outputId": "2e770c96-a912-47d1-8a5f-e2e8f445b9b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Removing shifted features: ['Unnamed: 0']\n",
            "🔍 Starting hyperparameter tuning...\n",
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[50]\tvalid_0's multi_logloss: 0.738967\n",
            "[100]\tvalid_0's multi_logloss: 0.459056\n",
            "[150]\tvalid_0's multi_logloss: 0.341589\n",
            "[200]\tvalid_0's multi_logloss: 0.285664\n",
            "[250]\tvalid_0's multi_logloss: 0.255238\n",
            "[300]\tvalid_0's multi_logloss: 0.23775\n",
            "[350]\tvalid_0's multi_logloss: 0.226354\n",
            "[400]\tvalid_0's multi_logloss: 0.219594\n",
            "[450]\tvalid_0's multi_logloss: 0.214511\n",
            "[500]\tvalid_0's multi_logloss: 0.211553\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[498]\tvalid_0's multi_logloss: 0.211526\n",
            "✅ Submission file created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from scipy.stats import randint, uniform, ks_2samp\n",
        "from sklearn.cluster import KMeans\n",
        "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
        "\n",
        "# ===================== 🔹 Setup =====================\n",
        "os.environ['LOKY_MAX_CPU_COUNT'] = '6'\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"lightgbm\")\n",
        "\n",
        "# ===================== 🔹 Load Data =====================\n",
        "df_train = pd.read_csv(\"hacktrain.csv\")\n",
        "df_test = pd.read_csv(\"hacktest.csv\")\n",
        "ID = df_test['ID']\n",
        "\n",
        "# ===================== 🔹 Target Encoding =====================\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(df_train['class'])\n",
        "\n",
        "X_train_df = df_train.drop(['ID', 'class'], axis=1)\n",
        "X_test_df = df_test.drop(['ID'], axis=1)\n",
        "\n",
        "# ===================== 🔹 Detect & Drop Shifted Features =====================\n",
        "X_train_df['__source'] = 0\n",
        "X_test_df['__source'] = 1\n",
        "combined = pd.concat([X_train_df, X_test_df], axis=0)\n",
        "\n",
        "shifted_features = []\n",
        "for col in X_train_df.columns:\n",
        "    if col == '__source':\n",
        "        continue\n",
        "    stat, pval = ks_2samp(\n",
        "        combined[combined['__source'] == 0][col],\n",
        "        combined[combined['__source'] == 1][col]\n",
        "    )\n",
        "    if pval < 0.01:\n",
        "        shifted_features.append(col)\n",
        "\n",
        "print(f\"⚠️ Removing shifted features: {shifted_features}\")\n",
        "X_train_df.drop(columns=shifted_features + ['__source'], inplace=True)\n",
        "X_test_df.drop(columns=shifted_features + ['__source'], errors='ignore', inplace=True)\n",
        "\n",
        "# ===================== 🔹 Imputation =====================\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_imputed = imputer.fit_transform(X_train_df)\n",
        "X_test_imputed = imputer.transform(X_test_df)\n",
        "\n",
        "# ===================== 🔹 Add Cluster Feature =====================\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "X_train_cluster = kmeans.fit_predict(X_train_imputed)\n",
        "X_test_cluster = kmeans.predict(X_test_imputed)\n",
        "\n",
        "# ===================== 🔹 Feature Engineering =====================\n",
        "def add_stat_features(X, cluster):\n",
        "    df = pd.DataFrame(X)\n",
        "    df['mean'] = df.mean(axis=1)\n",
        "    df['std'] = df.std(axis=1)\n",
        "    df['min'] = df.min(axis=1)\n",
        "    df['max'] = df.max(axis=1)\n",
        "    df['range'] = df['max'] - df['min']\n",
        "    df['skew'] = df.skew(axis=1)\n",
        "    df['cluster'] = cluster\n",
        "    return df.values\n",
        "\n",
        "X_train_final = add_stat_features(X_train_imputed, X_train_cluster)\n",
        "X_test_final = add_stat_features(X_test_imputed, X_test_cluster)\n",
        "\n",
        "# ===================== 🔹 Polynomial Features =====================\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
        "X_train_final = poly.fit_transform(X_train_final)\n",
        "X_test_final = poly.transform(X_test_final)\n",
        "\n",
        "# ===================== 🔹 Scale Before PCA =====================\n",
        "scaler = StandardScaler()\n",
        "X_train_final = scaler.fit_transform(X_train_final)\n",
        "X_test_final = scaler.transform(X_test_final)\n",
        "\n",
        "# ===================== 🔹 PCA =====================\n",
        "pca = PCA(n_components=0.99)\n",
        "X_train_final = pca.fit_transform(X_train_final)\n",
        "X_test_final = pca.transform(X_test_final)\n",
        "\n",
        "# ===================== 🔹 Split =====================\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_final, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
        ")\n",
        "\n",
        "# ===================== 🔹 Sample Weight =====================\n",
        "sample_weight = compute_sample_weight('balanced', y_train_split)\n",
        "\n",
        "# ===================== 🔹 Model & Tuning =====================\n",
        "model = LGBMClassifier(objective='multiclass',\n",
        "                       num_class=len(np.unique(y_train)),\n",
        "                       boosting_type='gbdt',\n",
        "                       random_state=42, verbose=-1)\n",
        "\n",
        "param_dist = {\n",
        "    'num_leaves': randint(30, 150),\n",
        "    'max_depth': [5, 7, 15],\n",
        "    'learning_rate': uniform(0.01, 0.1),\n",
        "    'n_estimators': randint(300, 800),\n",
        "    'subsample': uniform(0.6, 0.3),\n",
        "    'colsample_bytree': uniform(0.6, 0.3),\n",
        "    'reg_alpha': uniform(0.01, 0.5),\n",
        "    'reg_lambda': uniform(0.01, 0.5),\n",
        "    'min_child_samples': randint(5, 25),\n",
        "    'subsample_freq': [0, 1, 2]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,\n",
        "    scoring='f1_macro',\n",
        "    cv=cv,\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"🔍 Starting hyperparameter tuning...\")\n",
        "random_search.fit(X_train_split, y_train_split, sample_weight=sample_weight)\n",
        "\n",
        "# ===================== 🔹 Final Model Training =====================\n",
        "best_model = random_search.best_estimator_\n",
        "best_model.set_params(n_estimators=500, learning_rate=0.02)\n",
        "\n",
        "best_model.fit(\n",
        "    X_train_split, y_train_split,\n",
        "    sample_weight=sample_weight,\n",
        "    eval_set=[(X_val_split, y_val_split)],\n",
        "    eval_metric=\"multi_logloss\",\n",
        "    callbacks=[early_stopping(stopping_rounds=50), log_evaluation(50)]\n",
        ")\n",
        "\n",
        "# ===================== 🔹 Final Prediction =====================\n",
        "y_pred = best_model.predict(X_test_final)\n",
        "y_decoded = label_encoder.inverse_transform(y_pred)\n",
        "\n",
        "submission = pd.DataFrame({'ID': ID, 'class': y_decoded})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"✅ Submission file created successfully!\")"
      ],
      "metadata": {
        "id": "wBu73M7FZutz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "0e6a113c-2829-44c4-960b-31cd8fd04489"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'hacktrain.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3717618357>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# ===================== 🔹 Load Data =====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hacktrain.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hacktest.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hacktrain.csv'"
          ]
        }
      ]
    }
  ]
}